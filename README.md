# ğŸ©º Retinal Blood Vessel Segmentation

Multi-dataset retinal vessel segmentation project using:

* DRIVE
* CHASEDB1
* HRF

---

ğŸ“¥ Dataset Sources

The datasets used in this project were downloaded from the official sources:

ğŸ”¹ DRIVE Dataset
https://drive.grand-challenge.org/

ğŸ”¹ CHASEDB1 Dataset
https://blogs.kingston.ac.uk/retinal/chasedb1/

ğŸ”¹ HRF Dataset
https://www5.cs.fau.de/research/data/fundus-images/

These datasets are publicly available for research purposes.

---

# ğŸ“ Project Structure

```
Retinal_Blood_Vessel_Segmentation/
â”‚
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ merged/          # Standardized merged dataset (generated)
â”‚   â”‚   â”œâ”€â”€ images/
â”‚   â”‚   â””â”€â”€ masks/
â”‚   â”‚
â”‚   â”œâ”€â”€ patches/         # Extracted training patches (generated)
â”‚   â”‚   â”œâ”€â”€ images/
â”‚   â”‚   â””â”€â”€ masks/
â”‚   â”‚
â”‚   â””â”€â”€ raw/             # Raw extracted datasets (please unzip here)
â”‚       â”œâ”€â”€ chase_db1/
â”‚       â”œâ”€â”€ drive/
â”‚       â””â”€â”€ hrf/
â”‚
â”œâ”€â”€ raw_data_files/      # Zipped datasets (tracked with Git LFS)
â”‚   â”œâ”€â”€ chase_db.zip
â”‚   â”œâ”€â”€ datasets.zip
â”‚   â””â”€â”€ hrf.zip
â”‚
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ standardize_datasets.ipynb
â”‚   â””â”€â”€ patching_datasets.ipynb
â”‚
â”œâ”€â”€ setup_data_dirs.sh   # Run this to generate data directory structure
â”œâ”€â”€ requirements.txt
â”œâ”€â”€ pyproject.toml
â””â”€â”€ README.md
```

---

# ğŸš€ Setup Instructions

## 1ï¸âƒ£ Clone the Repository

```bash
git clone <repo-url>
cd Retinal_Blood_Vessel_Segmentation
```

If using Git LFS:

```bash
git lfs pull
```

---

## 2ï¸âƒ£ Create Data Directory Structure

1. Make the script executable

```bash
chmod +x setup_data_dirs.sh
```
2. Generate data directory structure

```bash
./setup_data_dirs.sh
```

This creates:

```
data/
 â”œâ”€â”€ merged/
 â”œâ”€â”€ patches/
 â””â”€â”€ raw/
```

---

## 3ï¸âƒ£ Unzip Raw Datasets

Unzip the files inside `raw_data_files/` into `data/raw/` as follows:

### ğŸ”¹ CHASEDB1

```bash
unzip raw_data_files/chase_db.zip -d data/raw/chase_db1
```

Final location:

```
data/raw/chase_db1/
```

---

### ğŸ”¹ DRIVE

```bash
unzip raw_data_files/datasets.zip -d data/raw/drive
```

Final location:

```
data/raw/drive/
```

---

### ğŸ”¹ HRF

```bash
unzip raw_data_files/hrf.zip -d data/raw/hrf
```

Final location:

```
data/raw/hrf/
```

---

# ğŸ§  Notebook Execution Order

Run notebooks in the following order:

---

## Step 1 â€” Standardize & Merge Datasets

Open and run all cells:

```
src/standardize_datasets.ipynb
```

This will:

* Merge DRIVE, CHASEDB1, HRF
* Resize to 512Ã—512
* Convert masks to binary
* Save output to:

```
data/merged/
 â”œâ”€â”€ images/
 â””â”€â”€ masks/
```

---

## Step 2 â€” Patch Extraction

Open and run all cells:

```
src/patching_datasets.ipynb
```

This will:

* Extract 256Ã—256 patches
* Save to:

```
data/patches/
 â”œâ”€â”€ images/
 â””â”€â”€ masks/
```

Expected patch count:
~1000+ samples

---

# ğŸ“Œ Important Notes

* `data/merged/` and `data/patches/` are generated folders.
* Do NOT manually modify them.
* Re-run notebooks if regeneration is required.

---

# ğŸ“¦ Dependencies

Install required packages:

```bash
pip install -r requirements.txt
```

or if using `uv`:

```bash
uv pip install -r requirements.txt
```

---

# ğŸ¯ Final Output Pipeline

```
Raw Data â†’ Standardization â†’ Merged Dataset â†’ Patch Extraction â†’ Model Training
```

---

# Model Training

- Please use the data generated and saved under `data/patches/` for training.

- You can add augmentations if more data is needed.
---
